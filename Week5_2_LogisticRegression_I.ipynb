{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YDDeE8oXDh6C"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# data imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "\n",
        "# modeling imports\n",
        "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression # Linear Regression Model\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder #Z-score variables\n",
        "from sklearn.metrics import mean_squared_error, f1_score,r2_score, roc_auc_score,recall_score,mean_absolute_percentage_error, mean_absolute_error, accuracy_score,precision_score #model evaluation\n",
        "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
        "\n",
        "# pipeline imports\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rINbeQu7Dh6F"
      },
      "source": [
        "# Review\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROw6PerHDh6G"
      },
      "source": [
        "\n",
        "## Link Functions\n",
        "\n",
        "In our Linear Regression lectures, we talked about adding non-linearity through **Feature Engineering**, but that's not the only way! We can also use **link functions** to add non-linearity.\n",
        "\n",
        "Link functions are just algebra we do to the linear prediction ($\\mathbf{X}\\beta$) in order to get the predicted value we *actually* want (e.g a probability).\n",
        "\n",
        "$$\\underbrace{y = \\mathbf{X}\\beta}_\\text{Linear Model}$$\n",
        "$$\\underbrace{y = g^{-1}(\\mathbf{X}\\beta)}_\\text{Generalized Linear Model}$$\n",
        "\n",
        "Oddly, we often specify our link function using it's *inverse*, hence the $g^{-1}()$ instead of $g()$. $g^{-1}()$ takes the linear prediction and transforms it into our desired predicted value. $g()$ takes our desired predicted value and transforms it back into our linear prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5VpfVteDh6H"
      },
      "source": [
        "\n",
        "### Logistic Regression\n",
        "In logistic regression, our goal is to predict a *probability* that a data point is in group `1`. We talked about using:\n",
        "\n",
        "- Linear Probability Models $g^{-1}: y = x$\n",
        "- Odds Models $g^{-1}: y = e^x$\n",
        "- Logistic Regression: $g^{-1}: y = \\frac{e^x}{1 + e^x}$\n",
        "\n",
        "Logistic Regression using the link function $g(x) = log{\\frac{x}{1-x}}$ and inverse link $g^{-1}: y = \\frac{e^x}{1 + e^x}$ gave us a great sigmoid shape that takes linear predictions ($y = \\mathbf{X}\\beta$) and turns them into predicted probabilities ($p = \\frac{e^{\\mathbf{X}\\beta}}{1 + e^{\\mathbf{X}\\beta}}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdspVBVKDh6H"
      },
      "source": [
        "## Maximum Likelihood Estimation\n",
        "\n",
        "Just like with Linear Regression, we can use **Maximum Likelihood Estimation** to choose the parameters (intercept and coefficients) of the model. But we have a different likelihood.\n",
        "\n",
        "In a linear regression, we assumed that our *errors* are *normally* distributed around the regression line. For logistic regression, we assume that our *errors* are *Bernoulli* distributed. The Bernoulli distribution is a discrete distribution (since our outcome is *discrete*, a.k.a categorical) that tells you the proability of being `0` or `1`.\n",
        "\n",
        "### Bernoulli Likelihood\n",
        "The formula for a Bernoulli distribution for a single data point $x$ is:\n",
        "\n",
        "$$ f(y;p(x)) = p(x)^{y} * (1-p(x))^{1-y}$$\n",
        "\n",
        "where $y$ is the group the data point belongs to (either `0` or `1`), and $p(x)$ is the predicted probability of that data point being a `1`.\n",
        "\n",
        "For example, let's say we're looking at the probability that it's sunny tomorrow. The predicted probability, according to the weather channel is $p(x) = 0.8$. The likelihood of it being sunny ($k = 1$) is:\n",
        "\n",
        "$$ f(1;0.8) = 0.8^1 * (1-0.8)^{1-1} = 0.8$$\n",
        "\n",
        "The likelihood of it not being sunny ($k = 0$) is:\n",
        "$$ f(0;0.8) = 0.8^0 * (1-0.8)^{1-0} = 0.2$$\n",
        "\n",
        "### Likelihood Function\n",
        "But we don't just have a SINGLE data point when fitting a logistic regression, we have MANY. So, we multiply the likelihood of each data point together to get the likelihood of the dataset:\n",
        "\n",
        "$$\\prod_{i = 1}^n p(x_i)^{y_i} * (1-p(x_i))^{1-y_i}$$\n",
        "\n",
        "We want to choose parameters (e.g. $\\beta_0$, or $\\beta_1$) that *maximize* this likelihood function. And how to we maximize it? We take it's (partial) derivatives and set them equal to zero!\n",
        "\n",
        "However, it turns out that its much easier to work with the *log* of this likelihood function, so we're often working with the *log* likelihood and taking it's derivatives (this will still find the optimal parameters for the model as the values that maximize the *log* likelihood will also maximize the likelihood):\n",
        "\n",
        "$$\\sum_{i = 1}^n y_i * log(p(x_i)) + (1-y_i) * log(1-p(x_i))$$\n",
        "\n",
        "### Loss Function\n",
        "Now it turns out, this log-loss is a really great **loss function** for logistic regression. Loss functions are metrics that\n",
        "\n",
        "1. measure the performance of your model, and\n",
        "2. have lower scores indicate better performing models\n",
        "\n",
        "Log-Loss (also called **Binary Cross Entropy**) does just that! Thus we often use it as a loss function for Logistic Regression.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1jZBGKZCaDfP8g3mFpt0FArPc1r7H60Bs\" alt=\"Q\" width = \"400\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vmOeKxADh6H"
      },
      "source": [
        "##  Logistic Regression Example\n",
        "\n",
        "Let's do a example from scratch using this [Breast Cancer](https://raw.githubusercontent.com/ywen2021/CPSC392/main/Data/BreastCancer.csv) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7-LluAobDh6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "160e55c9-f2e1-4ba5-f91a-8daafa036aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc       :  0.9494505494505494\n",
            "Train Prescision:  0.9565217391304348\n",
            "Train Recall    :  0.9058823529411765\n",
            "Train F1        :  0.9305135951661632\n",
            "Train ROC AUC   :  0.9893085655314757\n",
            "Test Acc        :  0.9298245614035088\n",
            "Test Prescision :  0.9473684210526315\n",
            "Test Recall     :  0.8571428571428571\n",
            "Test F1         :  0.9\n",
            "Test ROC AUC    :  0.9748677248677249\n"
          ]
        }
      ],
      "source": [
        "# read in data\n",
        "bc = pd.read_csv(\"https://raw.githubusercontent.com/ywen2021/CPSC392/main/Data/BreastCancer.csv\")\n",
        "### drop the unnamed last column\n",
        "bc.drop(bc.columns[bc.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "\n",
        "bc.head()\n",
        "bc['diagnosis'] = (bc['diagnosis'] == 'M').astype(int)\n",
        "bc\n",
        "# set X and y\n",
        "predictors = bc.columns[bc.columns.str.endswith(\"mean\")]\n",
        "\n",
        "X = bc[predictors]\n",
        "y = bc[\"diagnosis\"]\n",
        "\n",
        "\n",
        "# Train Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
        "# z-score\n",
        "\n",
        "pre = make_column_transformer((StandardScaler(), predictors),\n",
        "                              remainder = \"passthrough\")\n",
        "\n",
        "# Create Empty Model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "pipe = Pipeline([(\"pre\", pre), (\"model\", logreg)])\n",
        "\n",
        "\n",
        "\n",
        "# fit\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "# predict\n",
        "y_pred_train = pipe.predict(X_train)\n",
        "y_pred_test = pipe.predict(X_test)\n",
        "\n",
        "y_pred_train_prob = pipe.predict_proba(X_train)[:,1]\n",
        "y_pred_test_prob = pipe.predict_proba(X_test)[:,1]\n",
        "\n",
        "\n",
        "# assess\n",
        "# assess\n",
        "print(\"Train Acc       : \", accuracy_score(y_train, y_pred_train))\n",
        "print(\"Train Prescision: \", precision_score(y_train, y_pred_train))\n",
        "print(\"Train Recall    : \", recall_score(y_train, y_pred_train))\n",
        "print(\"Train F1        : \", f1_score(y_train, y_pred_train))\n",
        "print(\"Train ROC AUC   : \", roc_auc_score(y_train, y_pred_train_prob))\n",
        "\n",
        "\n",
        "print(\"Test Acc        : \", accuracy_score(y_test, y_pred_test))\n",
        "print(\"Test Prescision : \", precision_score(y_test, y_pred_test))\n",
        "print(\"Test Recall     : \", recall_score(y_test, y_pred_test))\n",
        "print(\"Test F1         : \", f1_score(y_test, y_pred_test))\n",
        "print(\"Test ROC AUC    : \", roc_auc_score(y_test, y_pred_test_prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RNszCPHDh6I"
      },
      "source": [
        "Quick note about Logistic Regression Coefficients:\n",
        "\n",
        "1. Logistic Regression coefficients are by default in terms of *log odds* meaning that they tell you how much the *predicted log odds* of being in group 1 will change when the *predictor* increases by 1-unit.\n",
        "2. In `sklearn`, Logistic Regression can actually handle outcomes with *more than 2* categories. For example we could predict what someone's major is using Logistic Regression. When we do that, `sklearn` actually builds *many* logistic regression models by default: one model per category (see [here](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) for more rigorous detail.) That means there are *multiple* sets of coefficients, one for each model. Because of this, when there is only 2 categories (e.g. a *binary* outcome), we use `[0]` to indicate that we want to grab only one set of coeficients, because there is only one set in the *binary* case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r62NY7SRDh6I"
      },
      "outputs": [],
      "source": [
        "coefficients = pd.DataFrame({\"Coef\": pipe.named_steps['logreg'].coef_[0], # grab array of coefficients\n",
        "                            \"Name\": predictors})\n",
        "intercept = pd.DataFrame({\"Coef\": pipe.named_steps['logreg'].intercept_[0], # grab intercept\n",
        "                                   \"Name\": \"intercept\"},\n",
        "                                   index = [coefficients.shape[0]]) # since this is only 1 row, assign row a row index\n",
        "\n",
        "coefficents_all = pd.concat([coefficients,intercept])\n",
        "coefficents_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agEHVCCYDh6I"
      },
      "source": [
        "# In Your Groups\n",
        "\n",
        "## Practice Code\n",
        "Now practice building Logistic Regression models in your groups. Using [this](https://raw.githubusercontent.com/ywen2021/CPSC392/main/Data/purchase.csv) dataset, build a Logistic Regression model that predicts whether or not customers signed up for a rewards program based on their age, income, and whether they had made a previous purchase. Make sure to z-score your continuous variables, and use an 80/20 Train-Test-Split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaAx6TpmDh6J"
      },
      "outputs": [],
      "source": [
        "# read in data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ywen2021/CPSC392/main/Data/purchase.csv\")\n",
        "\n",
        "df.head()\n",
        "\n",
        "# set X and y\n",
        "predictors = [\"age\", \"income_in_k\", \"previous_purchase\"]\n",
        "\n",
        "X = df[predictors]\n",
        "y = df[\"signed_up\"]\n",
        "\n",
        "# Train Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 392)\n",
        "\n",
        "# Z-score and Create Empty Model\n",
        "\n",
        "\n",
        "# fit\n",
        "\n",
        "\n",
        "# predict\n",
        "\n",
        "\n",
        "# assess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZOYPGoyDh6J"
      },
      "source": [
        "Now, grab the coefficients for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqwK8RVhDh6J"
      },
      "outputs": [],
      "source": [
        "# coefs\n",
        "coefficients = pd.DataFrame({\"Coef\": pipe.named_steps['logreg'].coef_[0], # grab array of coefficients\n",
        "                            \"Name\": predictors})\n",
        "intercept = pd.DataFrame({\"Coef\": pipe.named_steps['logreg'].intercept_[0], # grab intercept\n",
        "                                   \"Name\": \"intercept\"},\n",
        "                                   index = [coefficients.shape[0]]) # since this is only 1 row, assign row a row index\n",
        "\n",
        "coefficents_all = pd.concat([coefficients,intercept])\n",
        "coefficents_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91E82EWmDh6J"
      },
      "source": [
        "## The Problem with Logistic Regression Coefficients\n",
        "When you're presenting your Logistic Regression Models to non-data people, you might want to be able to tell them which variables have the biggest impact on the predicted value. Typically, we might use coefficients for this because they give us a single number that summarizes the relationship between our *predictors* and our *predicted value*.\n",
        "\n",
        "However, log odds are difficult to understand intuitively, especially if you're not a data person. Thus, we might want a different way to present our results. Luckily, if we *exponentiate* our **log odds** coefficients, we get **odds** coefficients. These are easier to understand, as most people understand intuitively what **odds** are.\n",
        "\n",
        "Remember, for **odds** the important threshold value is $1$. So any **odds** coefficient $>1$ has a direct/positive relationship with the outcome and anything with an **odds** coefficient $< 1$ has an inverse/negative relationship with the outcome.\n",
        "\n",
        "You can also use the **odds** coefs to give people an intuitive understanding of the relationship. If the odds coef is $2$ then increasing the predictor by 1-unit causes your predicted odds to *double*. Similarly, if the odds coef is $0.5$ then increasing the predictor by 1-unit causes your predicted odds to *halve*. If the odds coef is $1.25$ then increasing the predictor by 1-unit causes your predicted odds to increase by $25\\%$.\n"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}